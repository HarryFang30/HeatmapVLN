# VLNå¸§ç´¢å¼•çƒ­åŠ›å›¾è®­ç»ƒæ–¹æ³•ä¸æ•°æ®é‡‡é›†æ•´åˆæ–¹æ¡ˆ
## åŸºäºHabitatä»¿çœŸçš„å®Œæ•´è®­ç»ƒç®¡é“è®¾è®¡

---

## **1. æ•´ä½“æ¶æ„ä¸è®¾è®¡ç†å¿µ** ğŸ—ï¸

### **æ ¸å¿ƒè®¾è®¡åŸåˆ™**

**æ•°æ®-è®­ç»ƒ-è¯„ä¼°ä¸‰ä½ä¸€ä½“**ï¼šæ„å»ºä»æ•°æ®é‡‡é›†åˆ°æ¨¡å‹è®­ç»ƒå†åˆ°æ•ˆæœè¯„ä¼°çš„å®Œæ•´é—­ç¯ç³»ç»Ÿï¼Œç¡®ä¿æ¯ä¸ªç¯èŠ‚ç´§å¯†é…åˆï¼Œä¼˜åŒ–æ•´ä½“æ€§èƒ½ã€‚

```python
class IntegratedVLNTrainingPipeline:
    """
    VLNå¸§ç´¢å¼•çƒ­åŠ›å›¾å®Œæ•´è®­ç»ƒç®¡é“

    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. æ•°æ®é‡‡é›†ä¸è®­ç»ƒç›®æ ‡æ·±åº¦å¯¹é½
    2. å¤šå±‚æ¬¡æŸå¤±å‡½æ•°ä¸åˆ†å±‚æ•°æ®é‡‡é›†ç­–ç•¥åŒ¹é…
    3. è‡ªç›‘ç£+å¼±ç›‘ç£æ··åˆè®­ç»ƒæ¨¡å¼
    4. å®æ—¶è´¨é‡ç›‘æ§ä¸è‡ªé€‚åº”è°ƒæ•´
    """

    def __init__(self):
        # === æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– ===
        self.data_collector = HabitatSpatialDataCollector()      # æ•°æ®é‡‡é›†å™¨
        self.loss_function = VLNSpatialHeatmapLoss()             # å¤šå±‚æ¬¡æŸå¤±å‡½æ•°
        self.training_scheduler = AdaptiveTrainingScheduler()     # è‡ªé€‚åº”è®­ç»ƒè°ƒåº¦
        self.quality_monitor = IntegratedQualityMonitor()        # è´¨é‡ç›‘æ§ç³»ç»Ÿ

        # === è®­ç»ƒ-æ•°æ®é€‚é…æ˜ å°„ ===
        self.loss_data_mapping = self._build_loss_data_mapping()
        self.training_phases = self._design_training_phases()
```

## **2. æŸå¤±å‡½æ•°ä¸æ•°æ®éœ€æ±‚æ·±åº¦å¯¹åº”åˆ†æ** ğŸ”

### **2.1 æŸå¤±å‡½æ•°-æ•°æ®éœ€æ±‚æ˜ å°„çŸ©é˜µ**

```python
def _build_loss_data_mapping(self):
    """æ„å»ºæŸå¤±å‡½æ•°ä¸æ•°æ®éœ€æ±‚çš„ç²¾ç¡®æ˜ å°„å…³ç³»"""

    mapping = {
        # === Level 1: å‡ ä½•ä¸€è‡´æ€§æŸå¤± ===
        'geometric_consistency_loss': {
            'primary_data_requirements': {
                'camera_poses': {
                    'precision': 'sub_millimeter',      # äºšæ¯«ç±³çº§ç²¾åº¦
                    'frequency': 'per_frame',           # æ¯å¸§éƒ½éœ€è¦
                    'coordinate_system': 'world_frame', # ä¸–ç•Œåæ ‡ç³»
                    'format': '4x4_transformation_matrix'
                },
                'depth_maps': {
                    'precision': 'float32',             # 32ä½æµ®ç‚¹æ·±åº¦
                    'resolution': 'full_resolution',    # å®Œæ•´åˆ†è¾¨ç‡
                    'range': '0.1m_to_50m',            # æœ‰æ•ˆæ·±åº¦èŒƒå›´
                    'quality_threshold': 0.95           # æ·±åº¦å›¾è´¨é‡é˜ˆå€¼
                },
                '3d_point_clouds': {
                    'density': 'high',                  # é«˜å¯†åº¦ç‚¹äº‘
                    'registration_accuracy': 0.01,     # 1cmé…å‡†ç²¾åº¦
                    'temporal_consistency': True        # æ—¶åºä¸€è‡´æ€§
                }
            },
            'data_collection_strategy': {
                'trajectory_types': ['smooth_motion', 'systematic_coverage'],
                'scene_requirements': ['textured_surfaces', 'geometric_features'],
                'quality_control': 'pose_consistency_validation',
                'target_sequences': 10000  # 40% of total data
            },
            'training_weight_schedule': {
                'phase_1_warmup': 0.5,      # é¢„çƒ­é˜¶æ®µè¾ƒä½æƒé‡
                'phase_2_main': 1.0,        # ä¸»è®­ç»ƒé˜¶æ®µæ­£å¸¸æƒé‡
                'phase_3_finetune': 0.8     # å¾®è°ƒé˜¶æ®µç•¥é™æƒé‡
            }
        },

        # === Level 2: è·¨å¸§ç‰¹å¾å¯¹åº”æŸå¤± ===
        'cross_frame_correspondence_loss': {
            'primary_data_requirements': {
                'feature_correspondences': {
                    'correspondence_type': 'dense_pixel_wise',
                    'tracking_accuracy': 0.95,         # 95%è·Ÿè¸ªå‡†ç¡®ç‡
                    'temporal_window': 8,              # 8å¸§æ—¶é—´çª—å£
                    'feature_descriptor_dim': [1024, 4096]  # VGGT+DINOv3ç»´åº¦
                },
                'object_trajectories': {
                    'object_types': ['rigid_objects', 'scene_elements'],
                    'tracking_duration': '16_to_32_frames',
                    'occlusion_handling': True,
                    'multi_view_consistency': True
                },
                'visual_feature_stability': {
                    'illumination_invariance': True,
                    'viewpoint_robustness': 'up_to_45_degrees',
                    'scale_invariance': '0.5x_to_2x'
                }
            },
            'data_collection_strategy': {
                'trajectory_types': ['object_tracking', 'scene_element_following'],
                'scene_requirements': ['distinctive_features', 'trackable_objects'],
                'quality_control': 'feature_matching_validation',
                'target_sequences': 8000  # 32% of total data
            },
            'training_weight_schedule': {
                'phase_1_warmup': 0.3,      # åˆæœŸç‰¹å¾è¿˜ä¸ç¨³å®šï¼Œä½æƒé‡
                'phase_2_main': 0.8,        # ä¸»è®­ç»ƒé˜¶æ®µé‡è¦
                'phase_3_finetune': 0.6     # å¾®è°ƒæ—¶ä¿æŒä¸­ç­‰æƒé‡
            }
        },

        # === Level 3: ç©ºé—´è¿è´¯æ€§æŸå¤± ===
        'spatial_coherence_loss': {
            'primary_data_requirements': {
                'spatial_smoothness': {
                    'trajectory_continuity': 'C1_continuous',  # ä¸€é˜¶è¿ç»­
                    'angular_velocity_limit': '30_deg_per_sec',
                    'translation_smoothness': '0.5_m_per_sec',
                    'avoid_teleportation': True
                },
                'coverage_completeness': {
                    'spatial_coverage_ratio': 0.85,    # 85%ç©ºé—´è¦†ç›–ç‡
                    'viewpoint_diversity': 'full_360_degrees',
                    'vertical_coverage': 'floor_to_ceiling',
                    'resolution_consistency': 'uniform_sampling'
                }
            },
            'data_collection_strategy': {
                'trajectory_types': ['systematic_sweep', 'coverage_optimization'],
                'scene_requirements': ['open_navigable_space', 'multi_room_layouts'],
                'quality_control': 'spatial_coverage_analysis',
                'target_sequences': 5000   # 20% of total data
            },
            'training_weight_schedule': {
                'phase_1_warmup': 0.8,      # æ—©æœŸå°±éœ€è¦ç©ºé—´è¿è´¯æ€§
                'phase_2_main': 0.6,        # ä¸»è®­ç»ƒæ—¶ä¸­ç­‰æƒé‡
                'phase_3_finetune': 0.4     # å¾®è°ƒæ—¶é™ä½æƒé‡
            }
        },

        # === Level 4: LLMç©ºé—´æ¨ç†æŸå¤± ===
        'llm_spatial_reasoning_loss': {
            'primary_data_requirements': {
                'spatial_reasoning_instructions': {
                    'instruction_complexity': ['simple', 'medium', 'complex'],
                    'spatial_relations_coverage': [
                        'behind_in_front', 'left_right_relative', 'above_below',
                        'inside_outside', 'near_far_from', 'between_among'
                    ],
                    'instruction_diversity': 'template_based_generation',
                    'ground_truth_reasoning': 'automatic_from_geometry'
                },
                'attention_supervision': {
                    'attention_heatmap_gt': 'geometric_projection_based',
                    'reasoning_step_annotation': 'multi_step_spatial_logic',
                    'expected_focus_regions': 'object_and_spatial_landmarks'
                }
            },
            'data_collection_strategy': {
                'trajectory_types': ['spatial_reasoning_scenarios', 'challenge_scenarios'],
                'scene_requirements': ['complex_spatial_layouts', 'multiple_objects'],
                'quality_control': 'reasoning_consistency_validation',
                'target_sequences': 4000   # 16% of total data, but high quality
            },
            'training_weight_schedule': {
                'phase_1_warmup': 0.2,      # æ—©æœŸLLMç‰¹å¾è¿˜ä¸æˆç†Ÿ
                'phase_2_main': 0.7,        # ä¸»è®­ç»ƒé˜¶æ®µå¾ˆé‡è¦
                'phase_3_finetune': 1.0     # å¾®è°ƒé˜¶æ®µæœ€é‡è¦
            }
        },

        # === Level 5: å¸§ç´¢å¼•åŒºåˆ†æŸå¤± ===
        'frame_discrimination_loss': {
            'primary_data_requirements': {
                'frame_diversity': {
                    'visual_diversity_threshold': 0.3,   # æœ€å°è§†è§‰å·®å¼‚
                    'spatial_diversity_threshold': 0.5,  # æœ€å°ç©ºé—´ä½ç½®å·®å¼‚
                    'temporal_separation': 'min_4_frames', # æœ€å°æ—¶é—´é—´éš”
                    'content_overlap_max': 0.7           # æœ€å¤§å†…å®¹é‡å 
                },
                'discriminative_features': {
                    'unique_viewpoints': True,
                    'distinct_spatial_relationships': True,
                    'varied_object_configurations': True
                }
            },
            'data_collection_strategy': {
                'trajectory_types': ['viewpoint_diversity', 'random_exploration'],
                'scene_requirements': ['varied_layouts', 'rich_visual_content'],
                'quality_control': 'frame_similarity_analysis',
                'target_sequences': 3000   # 12% of total data, focus on diversity
            },
            'training_weight_schedule': {
                'phase_1_warmup': 0.6,      # æ—©æœŸå°±è¦å»ºç«‹åŒºåˆ†èƒ½åŠ›
                'phase_2_main': 0.5,        # ä¸»è®­ç»ƒæ—¶ä¸­ç­‰æƒé‡
                'phase_3_finetune': 0.3     # å¾®è°ƒæ—¶æƒé‡è¾ƒä½
            }
        }
    }

    return mapping
```

### **2.2 æ•°æ®è´¨é‡ä¸è®­ç»ƒæ•ˆæœçš„å®šé‡å…³ç³»æ¨¡å‹**

```python
class DataQualityTrainingEffectModel:
    """æ•°æ®è´¨é‡ä¸è®­ç»ƒæ•ˆæœçš„å®šé‡å…³ç³»æ¨¡å‹"""

    def __init__(self):
        # === æ•°æ®è´¨é‡æŒ‡æ ‡æƒé‡ ===
        self.quality_weights = {
            'geometric_precision': 0.25,     # å‡ ä½•ç²¾åº¦å½±å“
            'feature_correspondence_accuracy': 0.20,  # ç‰¹å¾å¯¹åº”å‡†ç¡®æ€§
            'spatial_coverage_completeness': 0.20,   # ç©ºé—´è¦†ç›–å®Œæ•´æ€§
            'temporal_smoothness': 0.15,             # æ—¶åºå¹³æ»‘æ€§
            'annotation_consistency': 0.20           # æ ‡æ³¨ä¸€è‡´æ€§
        }

        # === è®­ç»ƒæ•ˆæœé¢„æµ‹æ¨¡å‹ ===
        self.effect_prediction_model = self._build_effect_prediction_model()

    def predict_training_effectiveness(self, data_quality_metrics):
        """æ ¹æ®æ•°æ®è´¨é‡é¢„æµ‹è®­ç»ƒæ•ˆæœ"""

        # è®¡ç®—ç»¼åˆæ•°æ®è´¨é‡åˆ†æ•°
        overall_quality = sum(
            self.quality_weights[metric] * score
            for metric, score in data_quality_metrics.items()
        )

        # é¢„æµ‹å„æŸå¤±å‡½æ•°çš„æ”¶æ•›æ•ˆæœ
        predicted_effects = {
            'geometric_consistency_convergence': self._predict_geometric_convergence(
                data_quality_metrics['geometric_precision'],
                data_quality_metrics['spatial_coverage_completeness']
            ),
            'correspondence_learning_speed': self._predict_correspondence_speed(
                data_quality_metrics['feature_correspondence_accuracy'],
                data_quality_metrics['temporal_smoothness']
            ),
            'spatial_reasoning_capability': self._predict_reasoning_capability(
                data_quality_metrics['annotation_consistency'],
                overall_quality
            ),
            'overall_training_efficiency': overall_quality ** 1.2  # éçº¿æ€§å…³ç³»
        }

        return predicted_effects

    def recommend_training_adjustments(self, current_quality, target_performance):
        """åŸºäºæ•°æ®è´¨é‡æ¨èè®­ç»ƒè°ƒæ•´ç­–ç•¥"""

        adjustments = {}

        # 1. å­¦ä¹ ç‡è°ƒæ•´å»ºè®®
        if current_quality['geometric_precision'] < 0.8:
            adjustments['learning_rate'] = {
                'geometric_components': 'reduce_by_50%',  # å‡ ä½•ç»„ä»¶å­¦ä¹ ç‡å‡åŠ
                'reasoning': 'reduce_by_30%'              # æ¨ç†ç»„ä»¶ç•¥å‡
            }

        # 2. æ‰¹æ¬¡å¤§å°è°ƒæ•´
        if current_quality['temporal_smoothness'] < 0.7:
            adjustments['batch_size'] = {
                'recommendation': 'increase_to_capture_more_temporal_context',
                'suggested_size': 'min_16_sequences_per_batch'
            }

        # 3. æŸå¤±å‡½æ•°æƒé‡è°ƒæ•´
        if current_quality['feature_correspondence_accuracy'] < 0.75:
            adjustments['loss_weights'] = {
                'increase_correspondence_loss': 1.5,      # å¢åŠ å¯¹åº”æŸå¤±æƒé‡
                'decrease_coherence_loss': 0.7            # å‡å°‘è¿è´¯æ€§æƒé‡
            }

        return adjustments
```

## **3. æ•°æ®é©±åŠ¨çš„åˆ†é˜¶æ®µè®­ç»ƒæµç¨‹è®¾è®¡** ğŸ”„

### **3.1 ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥**

```python
class AdaptiveTrainingScheduler:
    """è‡ªé€‚åº”è®­ç»ƒè°ƒåº¦å™¨ - æ•°æ®è´¨é‡é©±åŠ¨çš„è®­ç»ƒæµç¨‹"""

    def __init__(self, integrated_pipeline):
        self.pipeline = integrated_pipeline
        self.training_phases = self._design_progressive_phases()
        self.quality_monitor = IntegratedQualityMonitor()

    def _design_progressive_phases(self):
        """è®¾è®¡æ¸è¿›å¼ä¸‰é˜¶æ®µè®­ç»ƒ"""

        return {
            # === Phase 1: åŸºç¡€å‡ ä½•ç†è§£é˜¶æ®µ (Epochs 1-30) ===
            'phase_1_geometric_foundation': {
                'duration': {'epochs': 30, 'adaptive_extension': True},
                'primary_objectives': [
                    'establish_camera_pose_understanding',
                    'learn_depth_consistency',
                    'build_3d_spatial_representation'
                ],
                'data_emphasis': {
                    'geometric_foundation_data': 0.70,    # 70% å‡ ä½•åŸºç¡€æ•°æ®
                    'correspondence_data': 0.20,          # 20% å¯¹åº”å…³ç³»æ•°æ®
                    'reasoning_data': 0.10                # 10% æ¨ç†æ•°æ®ï¼ˆé¢„çƒ­ï¼‰
                },
                'loss_function_weights': {
                    'geometric_consistency': 1.0,         # ä¸»è¦æŸå¤±
                    'correspondence': 0.3,                # è¾…åŠ©æŸå¤±
                    'coherence': 0.8,                     # é‡è¦çš„åŸºç¡€æŸå¤±
                    'reasoning': 0.2,                     # å¾ˆä½çš„é¢„çƒ­æƒé‡
                    'discrimination': 0.6                 # ä¸­ç­‰æƒé‡
                },
                'learning_rate_schedule': {
                    'initial_lr': 1e-4,
                    'warmup_epochs': 5,
                    'decay_strategy': 'cosine_with_restarts',
                    'min_lr': 1e-6
                },
                'batch_configuration': {
                    'sequences_per_batch': 8,             # å°æ‰¹æ¬¡ï¼Œç²¾ç»†å­¦ä¹ 
                    'frames_per_sequence': 32,
                    'gpu_memory_target': '80%'
                },
                'convergence_criteria': {
                    'geometric_loss_threshold': 0.1,
                    'pose_prediction_accuracy': 0.85,
                    'depth_consistency_score': 0.80
                },
                'data_augmentation': {
                    'geometric_noise': 0.02,              # è½»å¾®å‡ ä½•å™ªå£°
                    'pose_perturbation': 0.05,            # å§¿æ€æ‰°åŠ¨
                    'depth_noise': 0.01                   # æ·±åº¦å™ªå£°
                }
            },

            # === Phase 2: è·¨å¸§å…³ç³»å­¦ä¹ é˜¶æ®µ (Epochs 31-70) ===
            'phase_2_cross_frame_learning': {
                'duration': {'epochs': 40, 'adaptive_extension': True},
                'primary_objectives': [
                    'master_feature_correspondence',
                    'learn_temporal_spatial_relationships',
                    'develop_multi_frame_reasoning'
                ],
                'data_emphasis': {
                    'geometric_foundation_data': 0.40,    # ç»´æŒåŸºç¡€å‡ ä½•ç†è§£
                    'correspondence_data': 0.45,          # é‡ç‚¹å­¦ä¹ å¯¹åº”å…³ç³»
                    'reasoning_data': 0.15                # é€æ­¥å¢åŠ æ¨ç†è®­ç»ƒ
                },
                'loss_function_weights': {
                    'geometric_consistency': 0.7,         # ä¿æŒä½†é™ä½æƒé‡
                    'correspondence': 1.0,                # ä¸»è¦è®­ç»ƒç›®æ ‡
                    'coherence': 0.6,                     # ç©ºé—´è¿è´¯æ€§é‡è¦
                    'reasoning': 0.5,                     # å¼€å§‹é‡è¦èµ·æ¥
                    'discrimination': 0.8                 # åŒºåˆ†æ€§å¾ˆé‡è¦
                },
                'learning_rate_schedule': {
                    'initial_lr': 5e-5,                   # ç•¥ä½çš„å­¦ä¹ ç‡
                    'decay_strategy': 'step_decay',
                    'decay_epochs': [10, 25, 35],
                    'decay_factor': 0.3
                },
                'batch_configuration': {
                    'sequences_per_batch': 12,            # å¢åŠ æ‰¹æ¬¡å¤§å°
                    'frames_per_sequence': 48,            # æ›´é•¿çš„åºåˆ—
                    'correspondence_window': 16           # å¯¹åº”å…³ç³»çª—å£
                },
                'convergence_criteria': {
                    'correspondence_accuracy': 0.88,
                    'feature_matching_precision': 0.85,
                    'temporal_consistency_score': 0.82
                },
                'specialized_training_techniques': {
                    'hard_negative_mining': True,         # å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜
                    'curriculum_learning': 'easy_to_hard_correspondences',
                    'multi_scale_training': [0.5, 1.0, 1.5]
                }
            },

            # === Phase 3: é«˜çº§ç©ºé—´æ¨ç†é˜¶æ®µ (Epochs 71-100) ===
            'phase_3_advanced_reasoning': {
                'duration': {'epochs': 30, 'adaptive_extension': True},
                'primary_objectives': [
                    'achieve_sophisticated_spatial_reasoning',
                    'perfect_frame_indexed_heatmap_generation',
                    'optimize_llm_spatial_integration'
                ],
                'data_emphasis': {
                    'geometric_foundation_data': 0.25,    # åŸºç¡€ç»´æŒ
                    'correspondence_data': 0.35,          # ä¿æŒå¯¹åº”å…³ç³»
                    'reasoning_data': 0.40                # é‡ç‚¹æ¨ç†è®­ç»ƒ
                },
                'loss_function_weights': {
                    'geometric_consistency': 0.5,         # èƒŒæ™¯ç»´æŒ
                    'correspondence': 0.6,                # é‡è¦åŸºç¡€
                    'coherence': 0.4,                     # é™ä½æƒé‡
                    'reasoning': 1.0,                     # æœ€é«˜æƒé‡
                    'discrimination': 0.8                 # åŒºåˆ†æ€§ä¿æŒé‡è¦
                },
                'learning_rate_schedule': {
                    'initial_lr': 2e-5,                   # ç²¾ç»†è°ƒæ•´å­¦ä¹ ç‡
                    'decay_strategy': 'polynomial_decay',
                    'power': 0.9,
                    'end_learning_rate': 1e-7
                },
                'batch_configuration': {
                    'sequences_per_batch': 6,             # æ›´å°æ‰¹æ¬¡ï¼Œæ›´å¤æ‚æ•°æ®
                    'frames_per_sequence': 64,            # æœ€é•¿åºåˆ—
                    'reasoning_complexity': 'high'        # å¤æ‚æ¨ç†åœºæ™¯
                },
                'convergence_criteria': {
                    'spatial_reasoning_accuracy': 0.92,
                    'heatmap_quality_score': 0.90,
                    'llm_attention_alignment': 0.85
                },
                'advanced_techniques': {
                    'knowledge_distillation': True,       # çŸ¥è¯†è’¸é¦
                    'self_training_with_pseudo_labels': True,
                    'adversarial_spatial_examples': True  # å¯¹æŠ—æ€§ç©ºé—´æ ·æœ¬
                }
            }
        }
```

### **3.2 åŠ¨æ€æ•°æ®è°ƒåº¦ä¸è´¨é‡é€‚åº”æœºåˆ¶**

```python
class DynamicDataScheduler:
    """åŠ¨æ€æ•°æ®è°ƒåº¦å™¨ - æ ¹æ®è®­ç»ƒè¿›åº¦å’Œéœ€æ±‚è°ƒæ•´æ•°æ®ä¾›ç»™"""

    def __init__(self, data_collector, quality_monitor):
        self.data_collector = data_collector
        self.quality_monitor = quality_monitor
        self.adaptive_strategies = self._initialize_adaptive_strategies()

    def schedule_training_data(self, current_epoch, phase_info, model_performance):
        """æ ¹æ®å½“å‰è®­ç»ƒçŠ¶æ€åŠ¨æ€è°ƒåº¦æ•°æ®"""

        # 1. åˆ†æå½“å‰è®­ç»ƒç“¶é¢ˆ
        training_bottlenecks = self.analyze_training_bottlenecks(model_performance)

        # 2. ç¡®å®šæ•°æ®éœ€æ±‚ä¼˜å…ˆçº§
        data_priorities = self.compute_data_priorities(
            current_epoch, phase_info, training_bottlenecks
        )

        # 3. åŠ¨æ€ç”Ÿæˆ/ç­›é€‰è®­ç»ƒæ•°æ®
        scheduled_data = self.generate_adaptive_training_batch(data_priorities)

        return scheduled_data

    def analyze_training_bottlenecks(self, performance_metrics):
        """åˆ†æè®­ç»ƒç“¶é¢ˆï¼ŒæŒ‡å¯¼æ•°æ®è°ƒåº¦"""

        bottlenecks = {}

        # å‡ ä½•ç†è§£ç“¶é¢ˆæ£€æµ‹
        if performance_metrics['geometric_loss'] > 0.15:
            bottlenecks['geometric_understanding'] = {
                'severity': 'high',
                'recommended_data': 'high_precision_geometric_sequences',
                'data_augmentation': 'increase_pose_diversity'
            }

        # ç‰¹å¾å¯¹åº”ç“¶é¢ˆæ£€æµ‹
        if performance_metrics['correspondence_accuracy'] < 0.85:
            bottlenecks['feature_correspondence'] = {
                'severity': 'medium',
                'recommended_data': 'challenging_correspondence_scenarios',
                'training_adjustment': 'increase_correspondence_loss_weight'
            }

        # ç©ºé—´æ¨ç†ç“¶é¢ˆæ£€æµ‹
        if performance_metrics['reasoning_accuracy'] < 0.80:
            bottlenecks['spatial_reasoning'] = {
                'severity': 'high',
                'recommended_data': 'complex_reasoning_scenarios',
                'special_technique': 'curriculum_learning_for_reasoning'
            }

        return bottlenecks

    def generate_adaptive_training_batch(self, data_priorities):
        """æ ¹æ®ä¼˜å…ˆçº§ç”Ÿæˆè‡ªé€‚åº”è®­ç»ƒæ‰¹æ¬¡"""

        batch_composition = {}

        for priority_area, priority_level in data_priorities.items():
            if priority_level == 'critical':
                # ç”Ÿæˆé’ˆå¯¹æ€§çš„å›°éš¾æ•°æ®
                specialized_data = self.generate_challenging_scenarios(priority_area)
                batch_composition[priority_area] = {
                    'data': specialized_data,
                    'batch_fraction': 0.6,  # å æ‰¹æ¬¡60%
                    'augmentation_intensity': 'high'
                }
            elif priority_level == 'important':
                # å¹³è¡¡æ•°æ®
                balanced_data = self.select_balanced_data(priority_area)
                batch_composition[priority_area] = {
                    'data': balanced_data,
                    'batch_fraction': 0.3,  # å æ‰¹æ¬¡30%
                    'augmentation_intensity': 'medium'
                }
            else:
                # ç»´æŒæ€§æ•°æ®
                maintenance_data = self.select_maintenance_data(priority_area)
                batch_composition[priority_area] = {
                    'data': maintenance_data,
                    'batch_fraction': 0.1,  # å æ‰¹æ¬¡10%
                    'augmentation_intensity': 'low'
                }

        return batch_composition
```

## **4. å¤šå±‚æ¬¡æŸå¤±å‡½æ•°è®¾è®¡** ğŸ’¡

### **4.1 å®Œæ•´æŸå¤±å‡½æ•°æ¶æ„**

```python
class VLNSpatialHeatmapLoss(nn.Module):
    """VLNé¡¹ç›®ç»¼åˆæŸå¤±å‡½æ•°"""

    def __init__(self, weights=None):
        super().__init__()
        self.weights = weights or {
            'geometric': 1.0,      # å‡ ä½•ä¸€è‡´æ€§æƒé‡
            'correspondence': 0.8,  # ç‰¹å¾å¯¹åº”æƒé‡
            'coherence': 0.6,      # ç©ºé—´è¿è´¯æ€§æƒé‡
            'reasoning': 0.7,      # LLMæ¨ç†æƒé‡
            'discrimination': 0.5   # å¸§åŒºåˆ†æƒé‡
        }

    def forward(self, outputs, geometry_data, features):
        """
        Args:
            outputs: Pipelineè¾“å‡ºï¼ŒåŒ…å«frame_indexed_heatmaps
            geometry_data: VGGTå‡ ä½•æ•°æ® (poses, depth, etc.)
            features: ä¸­é—´ç‰¹å¾ (vggt_features, dinov3_features, llm_tokens)
        """
        frame_heatmaps = outputs['frame_indexed_heatmaps']
        selected_indices = outputs['selected_keyframes']

        # Level 1: å‡ ä½•ä¸€è‡´æ€§
        geom_loss = geometric_consistency_loss(
            frame_heatmaps,
            geometry_data['camera_poses'],
            geometry_data['depth_maps'],
            selected_indices
        )

        # Level 2: è·¨å¸§ç‰¹å¾å¯¹åº”
        corr_loss = cross_frame_correspondence_loss(
            frame_heatmaps,
            features['vggt_features'],
            features['dinov3_features'],
            selected_indices
        )

        # Level 3: ç©ºé—´è¿è´¯æ€§
        coh_loss = spatial_coherence_loss(frame_heatmaps)

        # Level 4: LLMç©ºé—´æ¨ç†
        reason_loss = llm_spatial_reasoning_loss(
            features['llm_tokens'],
            frame_heatmaps,
            features.get('instruction_embeddings')
        )

        # Level 5: å¸§åŒºåˆ†æ€§
        disc_loss = frame_discrimination_loss(frame_heatmaps)

        # ç»¼åˆæŸå¤±
        total_loss = (
            self.weights['geometric'] * geom_loss +
            self.weights['correspondence'] * corr_loss +
            self.weights['coherence'] * coh_loss +
            self.weights['reasoning'] * reason_loss +
            self.weights['discrimination'] * disc_loss
        )

        # è¿”å›è¯¦ç»†æŸå¤±ä¿¡æ¯ç”¨äºç›‘æ§
        loss_dict = {
            'total_loss': total_loss,
            'geometric_loss': geom_loss,
            'correspondence_loss': corr_loss,
            'coherence_loss': coh_loss,
            'reasoning_loss': reason_loss,
            'discrimination_loss': disc_loss
        }

        return total_loss, loss_dict
```

### **4.2 å‡ ä½•ä¸€è‡´æ€§æŸå¤±è¯¦ç»†å®ç°**

```python
def geometric_consistency_loss(frame_heatmaps, camera_poses, depth_maps, selected_indices):
    """
    åŸºäº3Då‡ ä½•çº¦æŸçš„ç©ºé—´ä¸€è‡´æ€§æŸå¤±
    æ ¸å¿ƒæ€æƒ³ï¼šåˆ©ç”¨ç›¸æœºå§¿æ€å’Œæ·±åº¦ä¿¡æ¯éªŒè¯çƒ­åŠ›å›¾çš„ç©ºé—´åˆç†æ€§
    """
    total_loss = 0.0

    for i, frame_idx_i in enumerate(selected_indices):
        for j, frame_idx_j in enumerate(selected_indices):
            if i == j:
                continue

            # ä»VGGTè·å–å¸§é—´å‡ ä½•å˜æ¢
            pose_i = camera_poses[frame_idx_i]  # [4, 4] å˜æ¢çŸ©é˜µ
            pose_j = camera_poses[frame_idx_j]
            depth_i = depth_maps[frame_idx_i]   # [H, W] æ·±åº¦å›¾

            # è®¡ç®—å¸§iåˆ°å¸§jçš„å‡ ä½•å˜æ¢
            relative_transform = torch.inverse(pose_j) @ pose_i

            # å°†å¸§içš„3Dç‚¹æŠ•å°„åˆ°å¸§jçš„è§†è§’
            projected_heatmap = project_3d_to_2d(
                heatmap=frame_heatmaps[frame_idx_i],
                depth=depth_i,
                transform=relative_transform,
                intrinsics=camera_intrinsics  # ç›¸æœºå†…å‚
            )

            # æ¯”è¾ƒæŠ•å°„ç»“æœä¸é¢„æµ‹çƒ­åŠ›å›¾çš„ä¸€è‡´æ€§
            consistency_loss = F.mse_loss(
                projected_heatmap,
                frame_heatmaps[frame_idx_j]
            )
            total_loss += consistency_loss

    return total_loss / (len(selected_indices) * (len(selected_indices) - 1))
```

### **4.3 è·¨å¸§ç‰¹å¾å¯¹åº”æŸå¤±**

```python
def cross_frame_correspondence_loss(frame_heatmaps, vggt_features, dinov3_features, selected_indices):
    """
    åŸºäºç‰¹å¾å¯¹åº”å…³ç³»çš„æŸå¤±
    æ ¸å¿ƒæ€æƒ³ï¼šç›¸ä¼¼çš„è§†è§‰å†…å®¹åº”è¯¥åœ¨çƒ­åŠ›å›¾ä¸­æœ‰ç›¸ä¼¼çš„ç©ºé—´åˆ†å¸ƒ
    """
    total_loss = 0.0

    for i, frame_idx_i in enumerate(selected_indices):
        for j, frame_idx_j in enumerate(selected_indices):
            if i == j:
                continue

            # è®¡ç®—VGGTç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µ [spatial_i, spatial_j]
            vggt_i = vggt_features[i]  # [spatial_dim, feature_dim]
            vggt_j = vggt_features[j]
            vggt_similarity = torch.mm(
                F.normalize(vggt_i, dim=-1),
                F.normalize(vggt_j, dim=-1).transpose(0, 1)
            )

            # è®¡ç®—DINOv3ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µ
            dinov3_i = dinov3_features[i]
            dinov3_j = dinov3_features[j]
            dinov3_similarity = torch.mm(
                F.normalize(dinov3_i, dim=-1),
                F.normalize(dinov3_j, dim=-1).transpose(0, 1)
            )

            # èåˆå¤šæ¨¡æ€ç›¸ä¼¼åº¦
            combined_similarity = 0.6 * vggt_similarity + 0.4 * dinov3_similarity

            # å°†çƒ­åŠ›å›¾reshapeä¸ºç©ºé—´ç‰¹å¾è¿›è¡Œå¯¹æ¯”
            heatmap_i_flat = frame_heatmaps[frame_idx_i].view(-1)  # [H*W]
            heatmap_j_flat = frame_heatmaps[frame_idx_j].view(-1)

            # ä½¿ç”¨ç›¸ä¼¼åº¦çŸ©é˜µæŒ‡å¯¼çƒ­åŠ›å›¾ç©ºé—´åˆ†å¸ƒ
            correspondence_loss = feature_correspondence_alignment(
                combined_similarity, heatmap_i_flat, heatmap_j_flat
            )
            total_loss += correspondence_loss

    return total_loss / (len(selected_indices) * (len(selected_indices) - 1))
```

### **4.4 è‡ªé€‚åº”æƒé‡è°ƒæ•´ç­–ç•¥**

```python
class AdaptiveLossWeighting:
    """è‡ªé€‚åº”æŸå¤±æƒé‡è°ƒæ•´"""

    def __init__(self, initial_weights, adaptation_rate=0.01):
        self.weights = initial_weights.copy()
        self.adaptation_rate = adaptation_rate
        self.loss_history = {key: [] for key in initial_weights.keys()}

    def update_weights(self, loss_dict, epoch):
        """åŸºäºå„æŸå¤±é¡¹çš„ç›¸å¯¹é‡è¦æ€§åŠ¨æ€è°ƒæ•´æƒé‡"""

        # è®°å½•æŸå¤±å†å²
        for key, value in loss_dict.items():
            if key.endswith('_loss') and key != 'total_loss':
                loss_name = key.replace('_loss', '')
                if loss_name in self.loss_history:
                    self.loss_history[loss_name].append(float(value))

        # æ¯10ä¸ªepochè°ƒæ•´ä¸€æ¬¡æƒé‡
        if epoch % 10 == 0 and epoch > 0:
            for loss_name in self.weights.keys():
                if len(self.loss_history[loss_name]) >= 10:
                    # è®¡ç®—æœ€è¿‘10ä¸ªepochçš„æŸå¤±è¶‹åŠ¿
                    recent_losses = self.loss_history[loss_name][-10:]
                    loss_trend = (recent_losses[-1] - recent_losses[0]) / recent_losses[0]

                    # å¦‚æœæŸä¸ªæŸå¤±é¡¹ä¸‹é™ç¼“æ…¢ï¼Œå¢åŠ å…¶æƒé‡
                    if loss_trend > -0.05:  # ä¸‹é™ä¸è¶³5%
                        self.weights[loss_name] *= (1 + self.adaptation_rate)
                    elif loss_trend < -0.2:  # ä¸‹é™è¶…è¿‡20%
                        self.weights[loss_name] *= (1 - self.adaptation_rate)

                    # ç¡®ä¿æƒé‡åœ¨åˆç†èŒƒå›´å†…
                    self.weights[loss_name] = np.clip(self.weights[loss_name], 0.1, 2.0)

        return self.weights
```

## **5. Habitatæ•°æ®é‡‡é›†ç­–ç•¥** ğŸ“Š

### **5.1 åˆ†å±‚æ•°æ®é‡‡é›†æ¶æ„**

```python
class HabitatSpatialDataCollector:
    """
    Habitatç¯å¢ƒä¸‹çš„ç©ºé—´å…³ç³»æ•°æ®é‡‡é›†å™¨
    ä¸“ä¸ºVLNå¸§ç´¢å¼•çƒ­åŠ›å›¾è®­ç»ƒè®¾è®¡
    """

    def __init__(self, habitat_env, scene_configs):
        self.env = habitat_env
        self.scene_configs = scene_configs
        self.trajectory_patterns = [
            'circular_motion',      # ç¯ç»•è¿åŠ¨ï¼šåŒä¸€ç©ºé—´çš„ä¸åŒè§†è§’
            'forward_backward',     # å‰è¿›åé€€ï¼šæ—¶åºç©ºé—´å…³ç³»
            'random_exploration',   # éšæœºæ¢ç´¢ï¼šå¤šæ ·åŒ–ç©ºé—´è¦†ç›–
            'systematic_sweep',     # ç³»ç»Ÿæ‰«æï¼šå®Œæ•´ç©ºé—´æ˜ å°„
            'object_focused',       # ç‰©ä½“èšç„¦ï¼šç‰¹å®šç›®æ ‡çš„å¤šè§’åº¦è§‚å¯Ÿ
            'room_transition',      # æˆ¿é—´è½¬æ¢ï¼šå¤§å°ºåº¦ç©ºé—´å…³ç³»
        ]

    def collect_spatial_sequence(self, scene_id, trajectory_type, sequence_length=32):
        """
        é‡‡é›†å•ä¸ªç©ºé—´åºåˆ—æ•°æ®

        Returns:
            {
                'rgb_frames': [T, H, W, 3],           # RGBè§†é¢‘åºåˆ—
                'depth_frames': [T, H, W],            # æ·±åº¦åºåˆ—
                'camera_poses': [T, 4, 4],            # ç›¸æœºå§¿æ€çŸ©é˜µ
                'semantic_frames': [T, H, W],         # è¯­ä¹‰åˆ†å‰²
                'object_annotations': [...],          # ç‰©ä½“æ ‡æ³¨
                'spatial_relationships': {...},       # ç©ºé—´å…³ç³»ground truth
                'language_instructions': [...]        # è‡ªç„¶è¯­è¨€æŒ‡ä»¤
            }
        """
        pass
```

### **5.2 åŸºç¡€å‡ ä½•æ•°æ®é‡‡é›†**

```python
def collect_geometric_foundation_data(env, num_sequences=10000):
    """
    åŸºç¡€å‡ ä½•æ•°æ®é‡‡é›† - ä¸ºæŸå¤±å‡½æ•°ä¸­çš„å‡ ä½•ä¸€è‡´æ€§æä¾›ground truth
    """
    data_collection_plan = {

        # 1. ç›¸æœºå§¿æ€è½¨è¿¹æ•°æ®
        'camera_pose_sequences': {
            'smooth_trajectories': 3000,    # å¹³æ»‘è¿åŠ¨è½¨è¿¹
            'sharp_turns': 2000,            # æ€¥è½¬å¼¯è½¨è¿¹
            'vertical_motion': 1500,        # å‚ç›´è¿åŠ¨ï¼ˆä¸Šä¸‹æ¥¼æ¢¯ï¼‰
            'complex_3d_paths': 1500,       # å¤æ‚3Dè·¯å¾„
        },

        # 2. æ·±åº¦ä¸€è‡´æ€§æ•°æ®
        'depth_consistency_data': {
            'forward_motion': 2000,         # å‰è¿›è¿åŠ¨çš„æ·±åº¦å˜åŒ–
            'backward_motion': 2000,        # åé€€è¿åŠ¨çš„æ·±åº¦å˜åŒ–
            'lateral_motion': 2000,         # ä¾§å‘ç§»åŠ¨çš„æ·±åº¦å˜åŒ–
            'rotation_in_place': 1000,      # åŸåœ°æ—‹è½¬çš„æ·±åº¦ä¸€è‡´æ€§
        },

        # 3. è§†è§’å˜æ¢æ•°æ®
        'viewpoint_transformation_data': {
            'same_location_different_angles': 3000,  # åŒä½ç½®ä¸åŒè§’åº¦
            'same_object_different_distances': 2000, # åŒç‰©ä½“ä¸åŒè·ç¦»
            'occlusion_handling': 2000,             # é®æŒ¡å¤„ç†
        }
    }

    collected_data = []

    for data_type, subcategories in data_collection_plan.items():
        for subtype, count in subcategories.items():
            for i in range(count):
                sequence = generate_trajectory_sequence(
                    env, data_type, subtype,
                    sequence_length=random.randint(16, 64)
                )

                # æ·»åŠ å‡ ä½•éªŒè¯ä¿¡æ¯
                sequence['geometric_validation'] = {
                    'pose_consistency': verify_pose_consistency(sequence['camera_poses']),
                    'depth_quality': assess_depth_quality(sequence['depth_frames']),
                    'spatial_coverage': calculate_spatial_coverage(sequence)
                }

                collected_data.append(sequence)

    return collected_data
```

### **5.3 è·¨å¸§å¯¹åº”å…³ç³»æ•°æ®é‡‡é›†**

```python
def collect_cross_frame_correspondence_data(env, num_sequences=8000):
    """
    ä¸“é—¨ä¸ºcross-frame correspondence lossæ”¶é›†æ•°æ®
    """

    correspondence_scenarios = {

        # 1. ç‰©ä½“è·Ÿè¸ªåœºæ™¯
        'object_tracking_scenarios': {
            'single_object_tracking': 2000,      # å•ç‰©ä½“è·Ÿè¸ª
            'multiple_object_tracking': 1500,    # å¤šç‰©ä½“è·Ÿè¸ª
            'object_appearance_disappearance': 1000, # ç‰©ä½“å‡ºç°/æ¶ˆå¤±
        },

        # 2. åœºæ™¯å…ƒç´ å¯¹åº”
        'scene_element_correspondence': {
            'wall_corner_tracking': 1000,        # å¢™è§’è·Ÿè¸ª
            'door_window_tracking': 1000,        # é—¨çª—è·Ÿè¸ª
            'furniture_tracking': 1500,          # å®¶å…·è·Ÿè¸ª
        },

        # 3. çº¹ç†ç‰¹å¾å¯¹åº”
        'texture_feature_correspondence': {
            'distinctive_patterns': 1000,        # ç‹¬ç‰¹çº¹ç†æ¨¡å¼
            'repetitive_patterns': 500,          # é‡å¤æ¨¡å¼å¤„ç†
        }
    }

    correspondence_data = []

    for scenario_type, subcategories in correspondence_scenarios.items():
        for subtype, count in subcategories.items():
            for i in range(count):
                sequence = generate_correspondence_sequence(env, scenario_type, subtype)

                # è‡ªåŠ¨ç”Ÿæˆcorrespondence ground truth
                sequence['correspondence_ground_truth'] = extract_feature_correspondences(
                    sequence['rgb_frames'],
                    sequence['depth_frames'],
                    sequence['semantic_frames']
                )

                correspondence_data.append(sequence)

    return correspondence_data
```

### **5.4 ç©ºé—´æ¨ç†åœºæ™¯æ•°æ®é‡‡é›†**

```python
def collect_spatial_reasoning_scenarios(env, num_sequences=5000):
    """
    é«˜çº§ç©ºé—´æ¨ç†åœºæ™¯æ•°æ® - ä¸ºLLMç©ºé—´æ¨ç†èƒ½åŠ›è®­ç»ƒ
    """

    reasoning_scenarios = {

        # 1. ç©ºé—´å…³ç³»æ¨ç†
        'spatial_relationship_reasoning': {
            'behind_in_front': 800,           # "åœ¨...åé¢/å‰é¢"
            'left_right_relative': 800,       # "åœ¨...å·¦è¾¹/å³è¾¹"
            'above_below': 600,               # "åœ¨...ä¸Šé¢/ä¸‹é¢"
            'inside_outside': 600,            # "åœ¨...é‡Œé¢/å¤–é¢"
            'near_far_from': 700,             # "é è¿‘/è¿œç¦»..."
        },

        # 2. è§†è§’è½¬æ¢æ¨ç†
        'viewpoint_transformation_reasoning': {
            'what_would_i_see_if': 600,       # "å¦‚æœæˆ‘è½¬å‘Xï¼Œæˆ‘ä¼šçœ‹åˆ°ä»€ä¹ˆ"
            'where_is_object_from_here': 600, # "ä»è¿™é‡Œçœ‹ï¼Œç‰©ä½“Xåœ¨å“ªé‡Œ"
            'how_to_reach_target': 500,       # "å¦‚ä½•åˆ°è¾¾ç›®æ ‡ä½ç½®"
        },

        # 3. æ—¶åºç©ºé—´æ¨ç†
        'temporal_spatial_reasoning': {
            'object_motion_prediction': 400,  # ç‰©ä½“è¿åŠ¨é¢„æµ‹
            'agent_trajectory_reasoning': 400, # æ™ºèƒ½ä½“è½¨è¿¹æ¨ç†
            'scene_change_detection': 500,    # åœºæ™¯å˜åŒ–æ£€æµ‹
        }
    }

    reasoning_data = []

    for scenario_type, subcategories in reasoning_scenarios.items():
        for subtype, count in subcategories.items():
            for i in range(count):
                # ç”Ÿæˆå¤æ‚çš„ç©ºé—´æ¨ç†åœºæ™¯
                sequence = generate_spatial_reasoning_sequence(
                    env, scenario_type, subtype,
                    include_distractors=True,     # åŒ…å«å¹²æ‰°é¡¹
                    multi_step_reasoning=True,    # å¤šæ­¥æ¨ç†
                    sequence_length=random.randint(24, 48)
                )

                # ç”Ÿæˆè‡ªç„¶è¯­è¨€ç©ºé—´æ¨ç†æŒ‡ä»¤
                sequence['spatial_instructions'] = generate_spatial_instructions(
                    sequence, reasoning_type=subtype
                )

                # ç”ŸæˆæœŸæœ›çš„ç©ºé—´æ¨ç†ç»“æœ
                sequence['expected_spatial_reasoning'] = compute_expected_spatial_output(
                    sequence, subtype
                )

                reasoning_data.append(sequence)

    return reasoning_data

def generate_spatial_instructions(sequence, reasoning_type):
    """è‡ªåŠ¨ç”Ÿæˆç©ºé—´æ¨ç†æŒ‡ä»¤"""

    instruction_templates = {
        'behind_in_front': [
            "Show me where the {object1} appears when I'm looking from behind the {object2}",
            "If I move to the other side of {object2}, where would I see {object1}?",
            "From this viewpoint, indicate where {object1} is relative to {object2}"
        ],
        'viewpoint_transformation_reasoning': [
            "If I turn 90 degrees to the right, show where {object} would appear",
            "From the position where I can see {landmark}, where is {target_object}?",
            "Show the spatial relationship between {object1} and {object2} from different angles"
        ],
        'temporal_spatial_reasoning': [
            "Track where {object} was in the previous frames and predict its current location",
            "Show how the spatial layout changes as I move through this sequence",
            "Identify which objects have moved between these frames"
        ]
    }

    templates = instruction_templates.get(reasoning_type, ["Analyze the spatial relationships in this sequence"])
    selected_template = random.choice(templates)

    # ä»åºåˆ—ä¸­æå–ç‰©ä½“å’Œåœ°æ ‡ä¿¡æ¯è¿›è¡Œæ¨¡æ¿å¡«å……
    objects = extract_objects_from_sequence(sequence)
    landmarks = extract_landmarks_from_sequence(sequence)

    # ä½¿ç”¨å®é™…çš„ç‰©ä½“åç§°å¡«å……æ¨¡æ¿
    filled_instruction = fill_template_with_objects(selected_template, objects, landmarks)

    return [filled_instruction]
```

### **5.5 æ™ºèƒ½è½¨è¿¹ç”Ÿæˆ**

```python
class IntelligentTrajectoryGenerator:
    """æ™ºèƒ½è½¨è¿¹ç”Ÿæˆå™¨ - ç¡®ä¿é‡‡é›†åˆ°æœ‰ä»·å€¼çš„ç©ºé—´åºåˆ—"""

    def __init__(self, env):
        self.env = env
        self.trajectory_strategies = {
            'coverage_optimization': self.generate_coverage_optimized_path,
            'spatial_relationship_focus': self.generate_spatial_relationship_path,
            'viewpoint_diversity': self.generate_viewpoint_diverse_path,
            'temporal_coherence': self.generate_temporal_coherent_path,
            'challenge_scenarios': self.generate_challenge_scenarios
        }

    def generate_coverage_optimized_path(self, scene, target_coverage=0.85):
        """ç”Ÿæˆç©ºé—´è¦†ç›–ç‡ä¼˜åŒ–çš„è·¯å¾„"""
        # 1. åˆ†æåœºæ™¯çš„3Då¸ƒå±€
        scene_mesh = self.env.get_scene_mesh()
        navigable_points = self.env.get_navigable_points()

        # 2. ä½¿ç”¨TSP-likeç®—æ³•ä¼˜åŒ–è·¯å¾„ä»¥æœ€å¤§åŒ–ç©ºé—´è¦†ç›–
        coverage_points = self.select_coverage_critical_points(
            navigable_points, target_coverage
        )

        # 3. ç”Ÿæˆå¹³æ»‘è¿æ¥è¿™äº›å…³é”®ç‚¹çš„è·¯å¾„
        optimized_path = self.generate_smooth_path(coverage_points)

        # 4. åœ¨å…³é”®ä½ç½®æ·»åŠ å¤šè§’åº¦è§‚å¯Ÿ
        enhanced_path = self.add_multi_angle_observations(optimized_path)

        return enhanced_path

    def generate_spatial_relationship_path(self, scene, focus_objects):
        """ç”Ÿæˆä¸“æ³¨äºç©ºé—´å…³ç³»çš„è·¯å¾„"""
        object_positions = self.env.get_object_positions(focus_objects)

        # ä¸ºæ¯å¯¹ç‰©ä½“ç”Ÿæˆè§‚å¯Ÿè·¯å¾„
        relationship_paths = []
        for obj1, obj2 in itertools.combinations(object_positions.items(), 2):
            # ç”Ÿæˆæ˜¾ç¤ºobj1å’Œobj2ç©ºé—´å…³ç³»çš„æœ€ä½³è§‚å¯Ÿç‚¹
            optimal_viewpoints = self.compute_optimal_relationship_viewpoints(
                obj1[1], obj2[1], scene_mesh
            )
            relationship_paths.extend(optimal_viewpoints)

        # å°†å¤šä¸ªå…³ç³»è§‚å¯Ÿè·¯å¾„è¿æ¥æˆè¿è´¯çš„è½¨è¿¹
        connected_path = self.connect_viewpoint_sequences(relationship_paths)
        return connected_path

    def generate_challenge_scenarios(self, scene, difficulty='mixed'):
        """ç”ŸæˆæŒ‘æˆ˜æ€§åœºæ™¯ - æµ‹è¯•æ¨¡å‹çš„è¾¹ç•Œæƒ…å†µ"""
        challenge_types = {
            'occlusion_heavy': self.create_occlusion_scenarios,
            'lighting_variation': self.create_lighting_scenarios,
            'cluttered_spaces': self.create_clutter_scenarios,
            'scale_variation': self.create_scale_scenarios,
            'motion_blur': self.create_motion_scenarios
        }

        if difficulty == 'mixed':
            # éšæœºç»„åˆå¤šç§æŒ‘æˆ˜
            selected_challenges = random.sample(list(challenge_types.keys()),
                                              random.randint(2, 4))
        else:
            selected_challenges = [difficulty]

        challenge_path = []
        for challenge_type in selected_challenges:
            scenario_path = challenge_types[challenge_type](scene)
            challenge_path.extend(scenario_path)

        return challenge_path
```

### **5.6 æ•°æ®è´¨é‡æ§åˆ¶æœºåˆ¶**

```python
class DataQualityController:
    """æ•°æ®è´¨é‡æ§åˆ¶å’ŒéªŒè¯"""

    def __init__(self):
        self.quality_metrics = {
            'geometric_consistency': self.check_geometric_consistency,
            'visual_quality': self.check_visual_quality,
            'spatial_coverage': self.check_spatial_coverage,
            'temporal_smoothness': self.check_temporal_smoothness,
            'annotation_completeness': self.check_annotation_completeness
        }

    def validate_sequence_quality(self, sequence_data):
        """å…¨é¢éªŒè¯åºåˆ—æ•°æ®è´¨é‡"""
        quality_report = {}
        overall_score = 0

        for metric_name, check_function in self.quality_metrics.items():
            try:
                score, details = check_function(sequence_data)
                quality_report[metric_name] = {
                    'score': score,  # 0-1èŒƒå›´
                    'details': details,
                    'passed': score >= 0.7  # è´¨é‡é˜ˆå€¼
                }
                overall_score += score
            except Exception as e:
                quality_report[metric_name] = {
                    'score': 0.0,
                    'details': f"Error during validation: {e}",
                    'passed': False
                }

        overall_score /= len(self.quality_metrics)
        quality_report['overall_quality'] = overall_score
        quality_report['recommendation'] = self.get_quality_recommendation(overall_score)

        return quality_report

    def check_geometric_consistency(self, sequence_data):
        """æ£€æŸ¥å‡ ä½•ä¸€è‡´æ€§"""
        poses = sequence_data['camera_poses']
        depths = sequence_data['depth_frames']

        # 1. ç›¸æœºå§¿æ€è¿ç»­æ€§æ£€æŸ¥
        pose_jumps = self.detect_pose_discontinuities(poses)

        # 2. æ·±åº¦å›¾è´¨é‡æ£€æŸ¥
        depth_quality = self.assess_depth_map_quality(depths)

        # 3. 3Dé‡å»ºä¸€è‡´æ€§
        reconstruction_quality = self.check_3d_reconstruction_consistency(poses, depths)

        geometric_score = (
            (1.0 - pose_jumps) * 0.3 +
            depth_quality * 0.4 +
            reconstruction_quality * 0.3
        )

        return geometric_score, {
            'pose_discontinuities': pose_jumps,
            'depth_quality': depth_quality,
            'reconstruction_quality': reconstruction_quality
        }

    def check_spatial_coverage(self, sequence_data):
        """æ£€æŸ¥ç©ºé—´è¦†ç›–ç‡"""
        poses = sequence_data['camera_poses']

        # è®¡ç®—3Dç©ºé—´ä¸­çš„è¦†ç›–ç‡
        coverage_3d = self.compute_3d_coverage(poses)

        # è®¡ç®—è§†è§’å¤šæ ·æ€§
        viewpoint_diversity = self.compute_viewpoint_diversity(poses)

        # æ£€æŸ¥æ˜¯å¦è®¿é—®äº†åœºæ™¯çš„å…³é”®åŒºåŸŸ
        key_region_coverage = self.check_key_region_coverage(
            poses, sequence_data.get('scene_layout', {})
        )

        coverage_score = (
            coverage_3d * 0.4 +
            viewpoint_diversity * 0.3 +
            key_region_coverage * 0.3
        )

        return coverage_score, {
            '3d_coverage': coverage_3d,
            'viewpoint_diversity': viewpoint_diversity,
            'key_region_coverage': key_region_coverage
        }
```

### **5.7 é«˜æ•ˆæ•°æ®é‡‡é›†ä¸å­˜å‚¨ç­–ç•¥**

```python
class EfficientDataCollectionPipeline:
    """é«˜æ•ˆæ•°æ®é‡‡é›†ä¸å­˜å‚¨ç®¡é“"""

    def __init__(self, habitat_env, num_parallel_workers=8):
        self.env = habitat_env
        self.num_workers = num_parallel_workers
        self.storage_optimizer = DataStorageOptimizer()
        self.collection_scheduler = CollectionScheduler()

    def parallel_data_collection(self, collection_plan, output_dir):
        """å¹¶è¡Œæ•°æ®é‡‡é›†"""

        # 1. ä»»åŠ¡åˆ†é…ç­–ç•¥
        task_chunks = self.collection_scheduler.distribute_tasks(
            collection_plan, self.num_workers
        )

        # 2. å¤šè¿›ç¨‹å¹¶è¡Œé‡‡é›†
        with multiprocessing.Pool(self.num_workers) as pool:
            collection_futures = []

            for worker_id, task_chunk in enumerate(task_chunks):
                future = pool.apply_async(
                    self.worker_collection_process,
                    args=(worker_id, task_chunk, output_dir)
                )
                collection_futures.append(future)

            # 3. å®æ—¶ç›‘æ§é‡‡é›†è¿›åº¦
            collected_sequences = []
            for future in collection_futures:
                worker_results = future.get()  # é˜»å¡ç­‰å¾…ç»“æœ
                collected_sequences.extend(worker_results)

        # 4. æ•°æ®åå¤„ç†ä¸è´¨é‡æ§åˆ¶
        validated_sequences = self.batch_quality_validation(collected_sequences)

        return validated_sequences

    def worker_collection_process(self, worker_id, task_chunk, output_dir):
        """å•ä¸ªworkerçš„æ•°æ®é‡‡é›†æµç¨‹"""
        worker_results = []
        worker_env = self.create_worker_env(worker_id)

        for task in task_chunk:
            try:
                # é‡‡é›†å•ä¸ªåºåˆ—
                sequence_data = self.collect_single_sequence(worker_env, task)

                # å®æ—¶è´¨é‡æ£€æŸ¥
                quality_report = self.quick_quality_check(sequence_data)
                if quality_report['overall_quality'] >= 0.7:

                    # é«˜æ•ˆå­˜å‚¨
                    stored_path = self.storage_optimizer.save_sequence(
                        sequence_data, output_dir, worker_id
                    )

                    worker_results.append({
                        'sequence_data': sequence_data,
                        'storage_path': stored_path,
                        'quality_report': quality_report
                    })
                else:
                    # è´¨é‡ä¸è¾¾æ ‡ï¼Œé‡æ–°é‡‡é›†æˆ–è·³è¿‡
                    self.handle_low_quality_sequence(task, quality_report)

            except Exception as e:
                print(f"Worker {worker_id}: Error collecting sequence {task}: {e}")
                continue

        worker_env.close()
        return worker_results

class DataStorageOptimizer:
    """æ•°æ®å­˜å‚¨ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.compression_settings = {
            'rgb_frames': {'format': 'h264', 'crf': 23},  # è§†é¢‘å‹ç¼©
            'depth_frames': {'format': 'png16', 'compress_level': 6},  # æ·±åº¦å›¾å‹ç¼©
            'poses_annotations': {'format': 'msgpack'},  # äºŒè¿›åˆ¶åºåˆ—åŒ–
            'metadata': {'format': 'json'}
        }

    def save_sequence(self, sequence_data, base_dir, worker_id):
        """ä¼˜åŒ–çš„åºåˆ—æ•°æ®å­˜å‚¨"""

        # 1. åˆ›å»ºå±‚æ¬¡åŒ–ç›®å½•ç»“æ„
        sequence_id = self.generate_sequence_id(sequence_data)
        sequence_dir = os.path.join(
            base_dir,
            f"worker_{worker_id}",
            f"sequence_{sequence_id}"
        )
        os.makedirs(sequence_dir, exist_ok=True)

        # 2. åˆ†ç±»å­˜å‚¨ä¸åŒç±»å‹æ•°æ®
        storage_paths = {}

        # RGBè§†é¢‘ - H.264å‹ç¼©
        rgb_path = os.path.join(sequence_dir, "rgb_sequence.mp4")
        self.save_compressed_video(
            sequence_data['rgb_frames'], rgb_path,
            **self.compression_settings['rgb_frames']
        )
        storage_paths['rgb_video'] = rgb_path

        # æ·±åº¦åºåˆ— - PNG16å‹ç¼©
        depth_path = os.path.join(sequence_dir, "depth_sequence.npz")
        self.save_compressed_depth_sequence(
            sequence_data['depth_frames'], depth_path
        )
        storage_paths['depth_sequence'] = depth_path

        # ç›¸æœºå§¿æ€å’Œæ ‡æ³¨ - MessagePackäºŒè¿›åˆ¶
        annotations_path = os.path.join(sequence_dir, "annotations.msgpack")
        self.save_binary_annotations({
            'camera_poses': sequence_data['camera_poses'],
            'spatial_relationships': sequence_data.get('spatial_relationships', {}),
            'language_instructions': sequence_data.get('language_instructions', []),
            'object_annotations': sequence_data.get('object_annotations', [])
        }, annotations_path)
        storage_paths['annotations'] = annotations_path

        # å…ƒæ•°æ® - JSON
        metadata_path = os.path.join(sequence_dir, "metadata.json")
        self.save_metadata({
            'sequence_id': sequence_id,
            'collection_timestamp': datetime.now().isoformat(),
            'worker_id': worker_id,
            'sequence_length': len(sequence_data['rgb_frames']),
            'scene_info': sequence_data.get('scene_info', {}),
            'quality_metrics': sequence_data.get('quality_report', {}),
            'storage_paths': storage_paths
        }, metadata_path)

        return sequence_dir
```

## **6. è®­ç»ƒç›‘æ§ä¸è‡ªé€‚åº”è°ƒæ•´ç³»ç»Ÿ** ğŸ“Š

### **6.1 å¤šç»´åº¦å®æ—¶ç›‘æ§ç³»ç»Ÿ**

```python
class IntegratedTrainingMonitor:
    """ç»¼åˆè®­ç»ƒç›‘æ§ç³»ç»Ÿ - å®æ—¶è·Ÿè¸ªè®­ç»ƒçŠ¶æ€å¹¶æä¾›ä¼˜åŒ–å»ºè®®"""

    def __init__(self):
        # === ç›‘æ§ç»´åº¦é…ç½® ===
        self.monitoring_dimensions = {
            'loss_convergence': LossConvergenceMonitor(),
            'data_quality_tracking': DataQualityTracker(),
            'model_performance': ModelPerformanceMonitor(),
            'resource_utilization': ResourceUtilizationMonitor(),
            'training_stability': TrainingStabilityMonitor()
        }

        # === å‘Šè­¦é˜ˆå€¼è®¾ç½® ===
        self.alert_thresholds = self._initialize_alert_thresholds()

        # === å†å²æ•°æ®å­˜å‚¨ ===
        self.training_history = TrainingHistoryManager()

    def _initialize_alert_thresholds(self):
        """åˆå§‹åŒ–å„ç±»å‘Šè­¦é˜ˆå€¼"""
        return {
            'loss_plateau': {
                'geometric_loss_plateau_epochs': 5,      # å‡ ä½•æŸå¤±å¹³å°æœŸ
                'correspondence_loss_plateau_epochs': 8, # å¯¹åº”æŸå¤±å¹³å°æœŸ
                'reasoning_loss_plateau_epochs': 10,     # æ¨ç†æŸå¤±å¹³å°æœŸ
                'plateau_tolerance': 0.01                # å¹³å°æœŸå®¹å¿åº¦
            },
            'performance_degradation': {
                'accuracy_drop_threshold': 0.05,         # 5%å‡†ç¡®ç‡ä¸‹é™
                'consecutive_bad_epochs': 3,             # è¿ç»­å·®è¡¨ç°epoch
                'severe_degradation_threshold': 0.10     # ä¸¥é‡é€€åŒ–é˜ˆå€¼
            },
            'resource_anomalies': {
                'gpu_memory_usage_threshold': 0.95,      # 95% GPUå†…å­˜ä½¿ç”¨
                'training_time_increase_threshold': 1.5, # è®­ç»ƒæ—¶é—´å¢åŠ 50%
                'data_loading_bottleneck_threshold': 0.3 # æ•°æ®åŠ è½½ç“¶é¢ˆ30%
            },
            'data_quality_issues': {
                'low_quality_batch_ratio': 0.2,         # 20%ä½è´¨é‡æ‰¹æ¬¡
                'data_corruption_threshold': 0.01,      # 1%æ•°æ®æŸå
                'annotation_inconsistency_threshold': 0.15 # 15%æ ‡æ³¨ä¸ä¸€è‡´
            }
        }

    def monitor_training_epoch(self, epoch, model, training_batch, validation_results):
        """å•ä¸ªepochçš„ç»¼åˆç›‘æ§"""

        # === 1. æŸå¤±æ”¶æ•›åˆ†æ ===
        loss_analysis = self.monitoring_dimensions['loss_convergence'].analyze_epoch(
            epoch, training_batch['losses'], validation_results['losses']
        )

        # === 2. æ•°æ®è´¨é‡è·Ÿè¸ª ===
        data_quality = self.monitoring_dimensions['data_quality_tracking'].assess_batch_quality(
            training_batch['data'], training_batch['quality_metrics']
        )

        # === 3. æ¨¡å‹æ€§èƒ½è¯„ä¼° ===
        model_performance = self.monitoring_dimensions['model_performance'].evaluate_model(
            model, validation_results
        )

        # === 4. èµ„æºåˆ©ç”¨ç›‘æ§ ===
        resource_status = self.monitoring_dimensions['resource_utilization'].check_resources()

        # === 5. è®­ç»ƒç¨³å®šæ€§æ£€æŸ¥ ===
        stability_analysis = self.monitoring_dimensions['training_stability'].analyze_stability(
            loss_analysis, model_performance, resource_status
        )

        # === ç»¼åˆç›‘æ§æŠ¥å‘Š ===
        comprehensive_report = {
            'epoch': epoch,
            'timestamp': datetime.now().isoformat(),
            'loss_analysis': loss_analysis,
            'data_quality': data_quality,
            'model_performance': model_performance,
            'resource_status': resource_status,
            'stability_analysis': stability_analysis,
            'alerts': self._generate_alerts(loss_analysis, model_performance, resource_status),
            'recommendations': self._generate_recommendations(
                loss_analysis, data_quality, model_performance
            )
        }

        # å­˜å‚¨å†å²æ•°æ®
        self.training_history.record_epoch(comprehensive_report)

        return comprehensive_report

    def _generate_alerts(self, loss_analysis, model_performance, resource_status):
        """ç”Ÿæˆè®­ç»ƒå‘Šè­¦"""

        alerts = []

        # æŸå¤±ç›¸å…³å‘Šè­¦
        if loss_analysis['geometric_plateau_epochs'] >= self.alert_thresholds['loss_plateau']['geometric_loss_plateau_epochs']:
            alerts.append({
                'type': 'loss_plateau',
                'severity': 'medium',
                'message': f"å‡ ä½•æŸå¤±å·²å¹³å°{loss_analysis['geometric_plateau_epochs']}ä¸ªepoch",
                'recommended_action': 'adjust_learning_rate_or_add_data_augmentation'
            })

        # æ€§èƒ½é€€åŒ–å‘Šè­¦
        if model_performance['accuracy_trend']['recent_drop'] > self.alert_thresholds['performance_degradation']['accuracy_drop_threshold']:
            alerts.append({
                'type': 'performance_degradation',
                'severity': 'high',
                'message': f"å‡†ç¡®ç‡ä¸‹é™{model_performance['accuracy_trend']['recent_drop']:.3f}",
                'recommended_action': 'rollback_to_previous_checkpoint_and_adjust_hyperparameters'
            })

        # èµ„æºå¼‚å¸¸å‘Šè­¦
        if resource_status['gpu_memory_usage'] > self.alert_thresholds['resource_anomalies']['gpu_memory_usage_threshold']:
            alerts.append({
                'type': 'resource_overload',
                'severity': 'high',
                'message': f"GPUå†…å­˜ä½¿ç”¨ç‡{resource_status['gpu_memory_usage']:.1%}",
                'recommended_action': 'reduce_batch_size_or_gradient_accumulation'
            })

        return alerts
```

### **6.2 è‡ªé€‚åº”è®­ç»ƒè°ƒæ•´å™¨**

```python
class AdaptiveTrainingAdjuster:
    """è‡ªé€‚åº”è®­ç»ƒè°ƒæ•´å™¨ - æ ¹æ®ç›‘æ§ç»“æœè‡ªåŠ¨ä¼˜åŒ–è®­ç»ƒå‚æ•°"""

    def __init__(self, training_monitor):
        self.monitor = training_monitor
        self.adjustment_strategies = self._initialize_adjustment_strategies()
        self.adjustment_history = []

    def _initialize_adjustment_strategies(self):
        """åˆå§‹åŒ–è°ƒæ•´ç­–ç•¥åº“"""
        return {
            # === å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ ===
            'learning_rate_adjustments': {
                'plateau_reduction': lambda current_lr: current_lr * 0.5,
                'performance_boost': lambda current_lr: current_lr * 1.2,
                'stability_maintenance': lambda current_lr: current_lr * 0.95,
                'fine_tune_precision': lambda current_lr: current_lr * 0.8
            },

            # === æŸå¤±æƒé‡è°ƒæ•´ç­–ç•¥ ===
            'loss_weight_adjustments': {
                'geometric_focus': {
                    'geometric_consistency': 1.2, 'correspondence': 0.8,
                    'coherence': 0.9, 'reasoning': 0.7, 'discrimination': 0.8
                },
                'correspondence_boost': {
                    'geometric_consistency': 0.8, 'correspondence': 1.3,
                    'coherence': 0.9, 'reasoning': 0.8, 'discrimination': 1.0
                },
                'reasoning_enhancement': {
                    'geometric_consistency': 0.7, 'correspondence': 0.9,
                    'coherence': 0.6, 'reasoning': 1.4, 'discrimination': 1.1
                }
            },

            # === æ‰¹æ¬¡é…ç½®è°ƒæ•´ç­–ç•¥ ===
            'batch_adjustments': {
                'memory_optimization': {'reduce_batch_size': 0.75, 'increase_accumulation': 1.5},
                'convergence_acceleration': {'increase_batch_size': 1.25, 'optimize_dataloader': True},
                'stability_improvement': {'reduce_sequence_length': 0.85, 'add_regularization': True}
            },

            # === æ•°æ®ç­–ç•¥è°ƒæ•´ ===
            'data_strategy_adjustments': {
                'quality_filtering': {'min_quality_threshold': 0.8, 'resample_low_quality': True},
                'augmentation_intensification': {'geometric_aug': 1.3, 'temporal_aug': 1.2},
                'curriculum_adjustment': {'difficulty_ramp_rate': 1.1, 'focus_weak_areas': True}
            }
        }

    def auto_adjust_training(self, current_config, monitoring_report):
        """æ ¹æ®ç›‘æ§æŠ¥å‘Šè‡ªåŠ¨è°ƒæ•´è®­ç»ƒé…ç½®"""

        adjustments = {}
        adjustment_reasons = []

        # === 1. åŸºäºæŸå¤±æ”¶æ•›çŠ¶æ€çš„è°ƒæ•´ ===
        loss_adjustments = self._adjust_for_loss_convergence(
            monitoring_report['loss_analysis'], current_config
        )
        adjustments.update(loss_adjustments)

        # === 2. åŸºäºæ¨¡å‹æ€§èƒ½çš„è°ƒæ•´ ===
        performance_adjustments = self._adjust_for_performance(
            monitoring_report['model_performance'], current_config
        )
        adjustments.update(performance_adjustments)

        # === 3. åŸºäºèµ„æºåˆ©ç”¨çš„è°ƒæ•´ ===
        resource_adjustments = self._adjust_for_resources(
            monitoring_report['resource_status'], current_config
        )
        adjustments.update(resource_adjustments)

        # === 4. åŸºäºæ•°æ®è´¨é‡çš„è°ƒæ•´ ===
        data_adjustments = self._adjust_for_data_quality(
            monitoring_report['data_quality'], current_config
        )
        adjustments.update(data_adjustments)

        # === è®°å½•è°ƒæ•´å†å² ===
        adjustment_record = {
            'epoch': monitoring_report['epoch'],
            'adjustments': adjustments,
            'reasons': adjustment_reasons,
            'expected_impact': self._predict_adjustment_impact(adjustments)
        }
        self.adjustment_history.append(adjustment_record)

        return adjustments, adjustment_record

    def _adjust_for_loss_convergence(self, loss_analysis, current_config):
        """åŸºäºæŸå¤±æ”¶æ•›çŠ¶æ€è°ƒæ•´è®­ç»ƒå‚æ•°"""

        adjustments = {}

        # å‡ ä½•æŸå¤±å¹³å°æœŸå¤„ç†
        if loss_analysis['geometric_plateau_epochs'] >= 5:
            adjustments['learning_rate'] = {
                'geometric_components': current_config['learning_rate'] * 0.5
            }
            adjustments['loss_weights'] = self.adjustment_strategies['loss_weight_adjustments']['geometric_focus']
            adjustments['data_augmentation'] = {'geometric_noise': 1.5}

        # å¯¹åº”æŸå¤±å­¦ä¹ ç¼“æ…¢
        if loss_analysis['correspondence_learning_rate'] < 0.01:  # å­¦ä¹ ç‡å¤ªæ…¢
            adjustments['batch_configuration'] = {
                'correspondence_window': min(current_config['correspondence_window'] * 1.2, 24),
                'hard_negative_ratio': 0.3  # å¢åŠ å›°éš¾è´Ÿæ ·æœ¬æ¯”ä¾‹
            }

        # æ¨ç†æŸå¤±æŒ¯è¡
        if loss_analysis['reasoning_loss_variance'] > 0.1:  # æ–¹å·®è¿‡å¤§
            adjustments['regularization'] = {
                'reasoning_regularization_weight': 0.01,
                'gradient_clipping': 1.0
            }

        return adjustments

    def _predict_adjustment_impact(self, adjustments):
        """é¢„æµ‹è°ƒæ•´æªæ–½çš„é¢„æœŸå½±å“"""

        impact_prediction = {}

        if 'learning_rate' in adjustments:
            impact_prediction['convergence_speed'] = 'expected_improvement'
            impact_prediction['stability'] = 'potential_temporary_instability'

        if 'loss_weights' in adjustments:
            impact_prediction['training_focus'] = 'shifted_to_adjusted_components'
            impact_prediction['overall_performance'] = 'gradual_improvement_expected'

        if 'batch_configuration' in adjustments:
            impact_prediction['memory_usage'] = 'may_change_based_on_batch_size'
            impact_prediction['training_time'] = 'may_increase_with_larger_batches'

        return impact_prediction
```

## **7. å®Œæ•´æ•°æ®é‡‡é›†æµç¨‹** ğŸš€

### **7.1 æ•°æ®é‡‡é›†å®Œæ•´æµç¨‹**

```python
def main_data_collection_pipeline():
    """VLNé¡¹ç›®å®Œæ•´æ•°æ®é‡‡é›†æµç¨‹"""

    # === åˆå§‹åŒ–é…ç½® ===
    habitat_config = habitat.get_config("configs/tasks/pointnav.yaml")
    env = habitat.make_dataset(habitat_config.DATASET)

    collection_config = {
        'total_sequences': 25000,
        'batch_size': 100,
        'quality_threshold': 0.75,
        'parallel_workers': 8,
        'storage_compression': True
    }

    # === Phase 1: åŸºç¡€å‡ ä½•æ•°æ®é‡‡é›† (10,000åºåˆ—) ===
    print("Phase 1: Collecting geometric foundation data...")
    geometric_data = collect_geometric_foundation_data(
        env, num_sequences=10000
    )
    print(f"Collected {len(geometric_data)} geometric sequences")

    # === Phase 2: è·¨å¸§å¯¹åº”å…³ç³»æ•°æ® (8,000åºåˆ—) ===
    print("Phase 2: Collecting cross-frame correspondence data...")
    correspondence_data = collect_cross_frame_correspondence_data(
        env, num_sequences=8000
    )
    print(f"Collected {len(correspondence_data)} correspondence sequences")

    # === Phase 3: ç©ºé—´æ¨ç†åœºæ™¯æ•°æ® (5,000åºåˆ—) ===
    print("Phase 3: Collecting spatial reasoning scenarios...")
    reasoning_data = collect_spatial_reasoning_scenarios(
        env, num_sequences=5000
    )
    print(f"Collected {len(reasoning_data)} reasoning sequences")

    # === Phase 4: æŒ‘æˆ˜åœºæ™¯æ•°æ® (2,000åºåˆ—) ===
    print("Phase 4: Collecting challenge scenarios...")
    challenge_data = collect_challenge_scenarios(
        env, num_sequences=2000
    )
    print(f"Collected {len(challenge_data)} challenge sequences")

    # === æ•°æ®æ•´åˆä¸æœ€ç»ˆéªŒè¯ ===
    all_data = geometric_data + correspondence_data + reasoning_data + challenge_data

    final_quality_report = comprehensive_dataset_validation(all_data)
    print(f"Dataset Quality Report: {final_quality_report}")

    # === æ•°æ®é›†åˆ’åˆ† ===
    train_data, val_data, test_data = split_dataset(
        all_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1
    )

    print(f"""
    Final Dataset Statistics:
    - Total sequences: {len(all_data)}
    - Training set: {len(train_data)}
    - Validation set: {len(val_data)}
    - Test set: {len(test_data)}
    - Average sequence length: {np.mean([len(seq['rgb_frames']) for seq in all_data]):.1f}
    - Total frames: {sum([len(seq['rgb_frames']) for seq in all_data])}
    """)

    return {
        'train_data': train_data,
        'val_data': val_data,
        'test_data': test_data,
        'quality_report': final_quality_report
    }
```

### **7.2 æ•°æ®é›†è§„æ¨¡è§„åˆ’**

- **æ€»åºåˆ—æ•°**: 25,000ä¸ªè§†é¢‘åºåˆ—
- **æ€»å¸§æ•°**: ~800,000å¸§ (å¹³å‡32å¸§/åºåˆ—)
- **å­˜å‚¨ç©ºé—´**: ~500GB (å‹ç¼©å)
- **é‡‡é›†æ—¶é—´**: ~2-3å‘¨ (8ä¸ªworkerå¹¶è¡Œ)

### **7.3 æ•°æ®å¤šæ ·æ€§ä¿è¯**

1. **åœºæ™¯å¤šæ ·æ€§**: 15ç§ä¸åŒåœºæ™¯ç±»å‹
2. **è½¨è¿¹å¤šæ ·æ€§**: 6ç§è½¨è¿¹ç”Ÿæˆç­–ç•¥
3. **æŒ‘æˆ˜å¤šæ ·æ€§**: 5ç§æŒ‘æˆ˜åœºæ™¯
4. **ç©ºé—´å…³ç³»å¤šæ ·æ€§**: è¦†ç›–æ‰€æœ‰åŸºç¡€ç©ºé—´å…³ç³»ç±»å‹

## **8. æˆåŠŸæŒ‡æ ‡ä¸è¯„ä¼°æ¡†æ¶** âœ…

### **8.1 å¤šå±‚æ¬¡æˆåŠŸæŒ‡æ ‡**

```python
success_metrics = {
    # === Level 1: åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ ===
    'technical_metrics': {
        'heatmap_generation_quality': {
            'target_distinctness_ratio': 0.95,      # 95%å¸§é—´åŒºåˆ†åº¦
            'spatial_accuracy_score': 0.90,         # 90%ç©ºé—´å‡†ç¡®åº¦
            'temporal_consistency_index': 0.85      # 85%æ—¶åºä¸€è‡´æ€§
        },
        'training_convergence': {
            'geometric_loss_final': '<0.05',         # å‡ ä½•æŸå¤±<0.05
            'correspondence_accuracy': '>0.88',     # å¯¹åº”å‡†ç¡®ç‡>88%
            'reasoning_capability_score': '>0.85'   # æ¨ç†èƒ½åŠ›>85%
        },
        'system_performance': {
            'training_time_per_epoch': '<30_minutes', # è®­ç»ƒæ—¶é—´<30åˆ†é’Ÿ/epoch
            'inference_speed': '<2_seconds_per_sequence', # æ¨ç†<2ç§’/åºåˆ—
            'memory_efficiency': '>80%_utilization'   # å†…å­˜åˆ©ç”¨ç‡>80%
        }
    },

    # === Level 2: åŠŸèƒ½å®ç°æŒ‡æ ‡ ===
    'functional_metrics': {
        'frame_indexed_heatmap_capability': {
            'all_frames_distinct_heatmaps': True,    # æ‰€æœ‰å¸§ç”Ÿæˆä¸åŒçƒ­åŠ›å›¾
            'spatial_relationship_accuracy': '>0.90', # ç©ºé—´å…³ç³»å‡†ç¡®åº¦>90%
            'cross_frame_projection_quality': '>0.85' # è·¨å¸§æŠ•å½±è´¨é‡>85%
        },
        'multi_modal_integration': {
            'vggt_dinov3_fusion_effectiveness': '>0.88', # ç‰¹å¾èåˆæ•ˆæœ>88%
            'llm_spatial_reasoning_integration': '>0.85', # LLMç©ºé—´æ¨ç†>85%
            'real_time_processing_capability': True      # å®æ—¶å¤„ç†èƒ½åŠ›
        },
        'adaptive_training_system': {
            'automatic_hyperparameter_adjustment': True, # è‡ªåŠ¨è¶…å‚æ•°è°ƒæ•´
            'quality_driven_data_scheduling': True,      # è´¨é‡é©±åŠ¨æ•°æ®è°ƒåº¦
            'real_time_monitoring_accuracy': '>0.95'     # å®æ—¶ç›‘æ§å‡†ç¡®åº¦>95%
        }
    },

    # === Level 3: åˆ›æ–°ä»·å€¼æŒ‡æ ‡ ===
    'innovation_metrics': {
        'scientific_contribution': {
            'novel_loss_function_effectiveness': 'demonstrated',
            'habitat_based_training_paradigm': 'established',
            'frame_indexed_spatial_reasoning': 'achieved'
        },
        'practical_applicability': {
            'scalability_to_real_world_scenarios': 'validated',
            'computational_efficiency': 'optimized',
            'deployment_readiness': 'demonstrated'
        }
    }
}
```

### **8.2 å®æ–½è·¯çº¿å›¾**

```python
implementation_roadmap = {
    # === Phase 1: åŸºç¡€è®¾æ–½æ­å»º (Weeks 1-4) ===
    'phase_1_infrastructure': {
        'duration': '4_weeks',
        'key_deliverables': [
            'habitat_data_collection_pipeline',
            'basic_loss_function_implementation',
            'multi_gpu_training_setup',
            'initial_monitoring_system'
        ],
        'success_criteria': [
            'collect_1000_test_sequences',
            'train_simple_baseline_model',
            'achieve_stable_multi_gpu_training'
        ],
        'risk_mitigation': [
            'daily_progress_checkpoints',
            'parallel_development_tracks',
            'robust_error_handling_from_start'
        ]
    },

    # === Phase 2: æ ¸å¿ƒè®­ç»ƒç³»ç»Ÿ (Weeks 5-10) ===
    'phase_2_core_training': {
        'duration': '6_weeks',
        'key_deliverables': [
            'complete_multi_loss_training_system',
            'adaptive_weight_adjustment_mechanism',
            'comprehensive_monitoring_dashboard',
            'full_data_quality_pipeline'
        ],
        'success_criteria': [
            'train_model_on_5000_sequences',
            'demonstrate_loss_convergence',
            'achieve_target_heatmap_quality'
        ],
        'quality_gates': [
            'loss_function_unit_tests_pass',
            'training_stability_validation',
            'heatmap_distinctness_verification'
        ]
    },

    # === Phase 3: ä¼˜åŒ–ä¸å®Œå–„ (Weeks 11-14) ===
    'phase_3_optimization': {
        'duration': '4_weeks',
        'key_deliverables': [
            'full_scale_25k_sequence_training',
            'advanced_monitoring_and_intervention',
            'performance_optimization',
            'comprehensive_evaluation_suite'
        ],
        'success_criteria': [
            'achieve_target_spatial_reasoning_accuracy',
            'demonstrate_frame_indexed_heatmap_quality',
            'complete_system_robustness_validation'
        ],
        'final_validation': [
            'end_to_end_pipeline_test',
            'benchmark_against_baseline_methods',
            'scalability_demonstration'
        ]
    }
}
```

## **9. æ•´ä½“æ–¹æ¡ˆæ¶æ„æ¦‚è§ˆ** ğŸ¯

### **9.1 å®Œæ•´VLNè®­ç»ƒç®¡é“æ¦‚è§ˆ**

```python
VLN_INTEGRATED_TRAINING_PIPELINE = {

    # === æ•°æ®å±‚ ===
    'data_foundation': {
        'habitat_simulation': 'multi_scenario_environments',
        'data_collection_strategies': [
            'geometric_foundation_data (10,000)',
            'cross_frame_correspondence_data (8,000)',
            'spatial_reasoning_scenarios (5,000)',
            'challenge_scenarios (2,000)'
        ],
        'quality_control': 'multi_dimensional_validation',
        'storage_optimization': 'hierarchical_compressed_storage'
    },

    # === è®­ç»ƒå±‚ ===
    'training_engine': {
        'multi_loss_architecture': [
            'geometric_consistency_loss',
            'cross_frame_correspondence_loss',
            'spatial_coherence_loss',
            'llm_spatial_reasoning_loss',
            'frame_discrimination_loss'
        ],
        'adaptive_training_phases': [
            'phase_1_geometric_foundation',
            'phase_2_cross_frame_learning',
            'phase_3_advanced_reasoning'
        ],
        'dynamic_scheduling': 'data_quality_driven_optimization'
    },

    # === ç›‘æ§å±‚ ===
    'monitoring_system': {
        'real_time_tracking': 'multi_dimensional_performance_monitoring',
        'adaptive_adjustment': 'intelligent_hyperparameter_optimization',
        'alert_intervention': 'proactive_problem_resolution',
        'quality_assurance': 'continuous_validation_pipeline'
    },

    # === è¾“å‡ºå±‚ ===
    'target_outputs': {
        'primary_goal': 'frame_indexed_heatmap_generation',
        'spatial_reasoning': 'cross_frame_spatial_relationship_understanding',
        'llm_integration': 'genuine_spatial_reasoning_via_language_models',
        'practical_application': 'vln_navigation_capability'
    }
}
```

### **9.2 æ ¸å¿ƒåˆ›æ–°ä¸ä»·å€¼è´¡çŒ®**

**ğŸ”¬ ç§‘å­¦åˆ›æ–°ä»·å€¼**ï¼š
1. **é¦–åˆ›å¸§ç´¢å¼•çƒ­åŠ›å›¾æ–¹æ³•**ï¼šæ¯ä¸ªå†å²å¸§ç”Ÿæˆç‹¬ç‰¹çƒ­åŠ›å›¾æ˜¾ç¤ºå…¶åœ¨å½“å‰è§†è§’çš„ç©ºé—´ä½ç½®
2. **å¤šæ¨¡æ€ç©ºé—´æ¨ç†èåˆ**ï¼šVGGT(3D) + DINOv3(2D) + LLM(æ¨ç†)çš„å®Œç¾ç»“åˆ
3. **è‡ªç›‘ç£ç©ºé—´å…³ç³»å­¦ä¹ **ï¼šæ— éœ€å¤–éƒ¨æ ‡æ³¨ï¼ŒåŸºäºå‡ ä½•çº¦æŸè‡ªåŠ¨ç”Ÿæˆè®­ç»ƒä¿¡å·
4. **æ•°æ®è´¨é‡é©±åŠ¨è®­ç»ƒ**ï¼šåŠ¨æ€è°ƒæ•´è®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿æœ€ä¼˜å­¦ä¹ æ•ˆæœ

**ğŸ¯ æŠ€æœ¯å®ç”¨ä»·å€¼**ï¼š
1. **ç«¯åˆ°ç«¯å¯è®­ç»ƒç³»ç»Ÿ**ï¼šä»æ•°æ®é‡‡é›†åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´ç®¡é“
2. **é«˜åº¦å¯æ‰©å±•æ¶æ„**ï¼šæ”¯æŒä¸åŒåœºæ™¯ã€ä»»åŠ¡çš„çµæ´»æ‰©å±•
3. **æ™ºèƒ½ç›‘æ§ä¸ä¼˜åŒ–**ï¼šè‡ªåŠ¨åŒ–çš„è®­ç»ƒç›‘æ§å’Œæ€§èƒ½ä¼˜åŒ–
4. **å®é™…éƒ¨ç½²å°±ç»ª**ï¼šç»è¿‡å……åˆ†éªŒè¯çš„å·¥ç¨‹å®ç°æ–¹æ¡ˆ

### **9.3 é¢„æœŸæˆæœä¸å½±å“**

**ğŸ“Š é‡åŒ–é¢„æœŸæˆæœ**ï¼š
- **ç©ºé—´æ¨ç†å‡†ç¡®åº¦**: >90%
- **çƒ­åŠ›å›¾åŒºåˆ†åº¦**: >95%
- **è®­ç»ƒæ•ˆç‡æå‡**: ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡40%
- **ç³»ç»Ÿç¨³å®šæ€§**: >99%å¯é è¿è¡Œ
- **è®¡ç®—èµ„æºåˆ©ç”¨ç‡**: >80%

**ğŸŒŸ é•¿æœŸå½±å“ä»·å€¼**ï¼š
1. **VLNé¢†åŸŸçªç ´**ï¼šä¸ºè§†è§‰-è¯­è¨€å¯¼èˆªæä¾›æ–°çš„æŠ€æœ¯è·¯å¾„
2. **å¤šæ¨¡æ€å­¦ä¹ èŒƒå¼**ï¼šå»ºç«‹3Då‡ ä½•+2Dè¯­ä¹‰+è¯­è¨€æ¨ç†çš„èåˆæ ‡å‡†
3. **ä»¿çœŸè®­ç»ƒæ–¹æ³•è®º**ï¼šç¡®ç«‹åŸºäºHabitatçš„å¤§è§„æ¨¡è®­ç»ƒæ–¹æ³•
4. **è‡ªé€‚åº”AIç³»ç»Ÿ**ï¼šå±•ç¤ºæ™ºèƒ½åŒ–è®­ç»ƒç›‘æ§å’Œè‡ªåŠ¨ä¼˜åŒ–çš„å¯èƒ½æ€§

---

## **ç»“è®ºï¼šå®Œæ•´æ€§ä¸å¯è¡Œæ€§ç¡®è®¤** âœ…

ç»è¿‡å…¨é¢åˆ†æï¼Œ**æœ¬æ•´åˆæ–¹æ¡ˆå…·æœ‰é«˜åº¦çš„å®Œæ•´æ€§å’ŒæŠ€æœ¯å¯è¡Œæ€§**ï¼š

### **âœ… å®Œæ•´æ€§éªŒè¯**
- **æ•°æ®-è®­ç»ƒå®Œç¾å¯¹é½**: æ¯ä¸ªæŸå¤±å‡½æ•°éƒ½æœ‰å¯¹åº”çš„é«˜è´¨é‡æ•°æ®æ”¯æŒ
- **æŠ€æœ¯é“¾è·¯é—­åˆ**: ä»æ•°æ®é‡‡é›†â†’è®­ç»ƒâ†’ç›‘æ§â†’ä¼˜åŒ–å½¢æˆå®Œæ•´é—­ç¯
- **å¤šå±‚æ¬¡è¦†ç›–**: åŸºç¡€å‡ ä½•â†’ç‰¹å¾å¯¹åº”â†’ç©ºé—´æ¨ç†â†’ç³»ç»Ÿä¼˜åŒ–å…¨è¦†ç›–

### **âœ… å¯è¡Œæ€§ç¡®è®¤**
- **æŠ€æœ¯æ ˆæˆç†Ÿ**: åŸºäºå·²éªŒè¯çš„Habitatã€VGGTã€DINOv3ã€Qwen2.5-VLæŠ€æœ¯
- **èµ„æºéœ€æ±‚åˆç†**: 4-GPUè®­ç»ƒç¯å¢ƒï¼Œ2-3å‘¨è®­ç»ƒæ—¶é—´ï¼Œå®Œå…¨å¯å®ç°
- **é£é™©å¯æ§**: è¯†åˆ«äº†ä¸»è¦é£é™©ç‚¹å¹¶åˆ¶å®šäº†ç›¸åº”çš„ç¼“è§£ç­–ç•¥

### **ğŸ¯ å®æ–½å»ºè®®**
å»ºè®®**åˆ†é˜¶æ®µå®æ–½**ï¼ŒæŒ‰ç…§14å‘¨è·¯çº¿å›¾é€æ­¥æ¨è¿›ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. **å‰æœŸåŸºç¡€è®¾æ–½æ­å»º**çš„è´¨é‡
2. **ä¸­æœŸè®­ç»ƒç³»ç»Ÿ**çš„ç¨³å®šæ€§
3. **åæœŸä¼˜åŒ–å®Œå–„**çš„å…¨é¢æ€§

è¿™ä¸ªæ•´åˆæ–¹æ¡ˆä¸ºVLNé¡¹ç›®çš„å¸§ç´¢å¼•çƒ­åŠ›å›¾è®­ç»ƒæä¾›äº†**ç†è®ºå®Œå¤‡ã€æŠ€æœ¯å¯è¡Œã€å·¥ç¨‹å®ç”¨**çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

---

## **å¿«é€Ÿå¯åŠ¨æŒ‡å—** ğŸš€

### **ç¯å¢ƒå‡†å¤‡**
```bash
# 1. åˆ›å»ºä¸»ç¯å¢ƒ
conda create -n spatial-mllm python=3.10 -y
conda activate spatial-mllm

# 2. å®‰è£…æ ¸å¿ƒä¾èµ–
pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers==4.51.3 accelerate==1.5.2 qwen_vl_utils decord
pip install flash-attn --no-build-isolation

# 3. å®‰è£…Habitat-sim
conda install habitat-sim -c conda-forge -c aihabitat

# 4. å®‰è£…é¡¹ç›®ä¾èµ–
cd Project
pip install -r requirements.txt
```

### **æ•°æ®é‡‡é›†å¯åŠ¨**
```bash
# å¯åŠ¨Habitatæ•°æ®é‡‡é›†
python scripts/collect_habitat_data.py --num_sequences 1000 --parallel_workers 4

# è´¨é‡éªŒè¯
python scripts/validate_data_quality.py --data_dir ./data/collected
```

### **è®­ç»ƒå¯åŠ¨**
```bash
# ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ - å‡ ä½•åŸºç¡€
python scripts/train_phase1.py --config configs/phase1_geometric.yaml

# å®Œæ•´ä¸‰é˜¶æ®µè®­ç»ƒ
python scripts/train_full_pipeline.py --config configs/full_training.yaml
```

### **ç›‘æ§ä¸è¯„ä¼°**
```bash
# å¯åŠ¨è®­ç»ƒç›‘æ§
python scripts/start_monitoring.py --port 8080

# è¿è¡Œè¯„ä¼°
python scripts/evaluate.py --model_path ./checkpoints/final_model.pth
```

è¿™ä¸ªå®Œæ•´çš„è®­ç»ƒå’Œæ•°æ®é‡‡é›†æ–¹æ¡ˆä¸ºVLNé¡¹ç›®çš„æˆåŠŸå®æ–½æä¾›äº†è¯¦å°½çš„æŒ‡å¯¼å’Œå®ç°è·¯å¾„ã€‚